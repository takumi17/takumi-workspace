{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードでは, 以下の3つのライブラリをインポートしています. \n",
    "\n",
    "* **`torch`**: 機械学習フレームワークであるPyTorchのライブラリです. ニューラルネットワークの構築や学習, 推論などに必要な機能を提供します. \n",
    "* **`torch.nn.functional`**: PyTorchの機能的なAPIを提供するモジュールです. 活性化関数や損失関数など, ニューラルネットワークの構築に役立つ関数を定義しています. \n",
    "* **`transformers`**: 自然言語処理タスクに特化したライブラリです. BERTやGPT-2などの事前学習済みモデルを簡単に利用することができます. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリを使用して, 事前学習済みのBERTモデルを読み込みます. \n",
    "\n",
    "**2.1 `model_name = \"bert-base-uncased\"`**\n",
    "\n",
    "この行は, 使用するBERTモデルの名前を `\"bert-base-uncased\"` に設定しています. \n",
    "\n",
    "* `\"bert-base-uncased\"` は, Google AIが公開しているBERTモデルの1つです. \n",
    "* \"uncased\" は, 大文字と小文字を区別しないモデルであることを意味します. \n",
    "\n",
    "**2.2 `tokenizer = BertTokenizer.from_pretrained(model_name)`**\n",
    "\n",
    "この行は, `BertTokenizer` クラスを使用して, `model_name` で指定されたモデルに対応するトークナイザを作成し, `tokenizer` 変数に代入しています. \n",
    "\n",
    "* トークナイザは, テキストをモデルが入力できる形式に変換する役割を担います. \n",
    "* BERTモデルでは, 単語をサブワードと呼ばれる小さな単位に分割し, それぞれにIDを割り当てる必要があります. \n",
    "\n",
    "**2.3 `bert_model = BertModel.from_pretrained(model_name)`**\n",
    "\n",
    "この行は, `BertModel` クラスを使用して, `model_name` で指定されたモデルを読み込み, `bert_model` 変数に代入しています. \n",
    "\n",
    "* BERTモデルは, Transformerと呼ばれるニューラルネットワークアーキテクチャに基づいて構築されています. \n",
    "* このモデルは, 大量のテキストデータで事前学習されており, 様々な自然言語処理タスクに利用することができます. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは、`bert_model` 変数に格納されているBERTモデルの**構成情報**を出力します。\n",
    "\n",
    "**1. 構成情報とは？**\n",
    "\n",
    "BERTモデルは、様々なハイパーパラメータによって構成されています。これらのハイパーパラメータは、モデルの性能や動作に影響を与えます。\n",
    "\n",
    "**2. 出力される情報**\n",
    "\n",
    "`print(bert_model.config)` を実行すると、以下の情報が出力されます。\n",
    "\n",
    "* **モデル名**: 使用されているBERTモデルの名前\n",
    "* **アーキテクチャ**: モデルのアーキテクチャ (例: `base`, `large`)\n",
    "* **層数**: モデルの層数\n",
    "* **隠れ層ユニット数**: 各層の隠れ層ユニット数\n",
    "* **中間層サイズ**: モデルの中間層サイズ\n",
    "* **Attention ヘッド数**: Attention ヘッドの数\n",
    "* **Attention キーサイズ**: Attention キーサイズ\n",
    "* **Attention 値サイズ**: Attention 値サイズ\n",
    "* **単語埋め込み次元数**: 単語埋め込み次元数\n",
    "* **最大入力トークン数**: 最大入力トークン数\n",
    "* **プーリング方法**: プーリング方法 (例: `last-avg`, `first-avg`)\n",
    "* **その他**: モデル固有のハイパーパラメータ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `bert_model` 変数に格納されているBERTモデルの**オブジェクトそのもの**を出力します. \n",
    "\n",
    "**1. オブジェクトとは？**\n",
    "\n",
    "Pythonでは, 様々なデータ型が存在します. その中でも, **オブジェクト** は, 属性とメソッドを持つデータ型です. \n",
    "\n",
    "* **属性**: オブジェクトが持つデータ\n",
    "* **メソッド**: オブジェクトが実行できる処理\n",
    "\n",
    "**2. BERTモデルオブジェクト**\n",
    "\n",
    "`bert_model` 変数に格納されているBERTモデルオブジェクトは, 以下の属性とメソッドを持っています. \n",
    "\n",
    "**属性**\n",
    "\n",
    "* `config`: モデルの構成情報\n",
    "* `state_dict`: モデルのパラメータ情報\n",
    "* `embeddings`: 単語埋め込み層\n",
    "* `encoder`: エンコーダー層\n",
    "* `pooler`: プーリング層\n",
    "* `dropout`: ドロップアウト層\n",
    "* `classifier`: 分類層\n",
    "* `loss`: 損失関数\n",
    "* `optimizer`: 最適化アルゴリズム\n",
    "\n",
    "**メソッド**\n",
    "\n",
    "* `forward(input_ids, attention_mask=None, token_type_ids=None)`: 入力テキストを処理し, 出力ベクトルを返す\n",
    "* `save_pretrained(save_directory)`: モデルを保存する\n",
    "* `load_state_dict(state_dict)`: モデルのパラメータを読み込む\n",
    "* `train()`: モデルを訓練モードに設定する\n",
    "* `eval()`: モデルを評価モードに設定する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"I'm reading a book\",\n",
    "             \"I'd like to book a room\",\n",
    "             \"I'd like to reserve a room\",\n",
    "             \"Please reserve this table for us\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 1005, 1049, 3752, 1037, 2338,  102,    0,    0],\n",
       "        [ 101, 1045, 1005, 1040, 2066, 2000, 2338, 1037, 2282,  102],\n",
       "        [ 101, 1045, 1005, 1040, 2066, 2000, 3914, 1037, 2282,  102],\n",
       "        [ 101, 3531, 3914, 2023, 2795, 2005, 2149,  102,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(text_list, max_length=10, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリの `tokenizer` を使って, `text_list` に含まれる英文をBERTモデルに入力できる形式に変換し, `encoding` 変数に代入しています. \n",
    "\n",
    "**1.1 `tokenizer(text_list)`**\n",
    "\n",
    "* `tokenizer`: `BertTokenizer` オブジェクト\n",
    "* `text_list`: 変換対象の英文リスト\n",
    "\n",
    "この部分は, `tokenizer` オブジェクトを使って, `text_list` に含まれる英文をトークン化します. \n",
    "\n",
    "**トークン化** とは, 英文を単語や記号などの小さな単位（トークン）に分割し, それぞれにIDを割り当てる処理です. BERTモデルは, トークン単位で処理を行うため, この工程が必要となります. \n",
    "\n",
    "**1.2 `max_length=10`**\n",
    "\n",
    "* `max_length`: 入力シーケンスの最大長\n",
    "\n",
    "このオプションは, 入力シーケンスの最大長を10トークンに設定します. BERTモデルは, 最大128トークンまでの入力シーケンスを処理できますが, 長い文章を処理すると計算量が多くなるため, このオプションで長さを制限することがあります. \n",
    "\n",
    "**1.3 `padding=\"max_length\"`**\n",
    "\n",
    "* `padding`: パディング方法\n",
    "\n",
    "このオプションは, 入力シーケンスの長さを `max_length` までパディング（ゼロ埋め）する方法を指定します. `padding=\"max_length\"` を指定すると, 短い文章は `max_length` までゼロで埋め込まれ, 長い文章は `max_length` で切り捨てられます. \n",
    "\n",
    "**1.4 `truncation=True`**\n",
    "\n",
    "* `truncation`: 文章を切断するかどうか\n",
    "\n",
    "このオプションは, 長い文章を `max_length` で切断するかどうかを指定します. `truncation=True` を指定すると, 長い文章は先頭から `max_length` トークン分だけ抽出され, それ以降の部分は切り捨てられます. \n",
    "\n",
    "**1.5 `return_tensors=\"pt\"`**\n",
    "\n",
    "* `return_tensors`: 出力データの形式\n",
    "\n",
    "このオプションは, 出力データの形式をPyTorchのテンソル形式に設定します. `return_tensors=\"pt\"` を指定すると, 出力データはPyTorchのテンソル形式で返されます. \n",
    "\n",
    "**2. `encoding` 変数の内容**\n",
    "\n",
    "`encoding` 変数には, 以下の情報が格納されたテンソルが入ります. \n",
    "\n",
    "* **input_ids**: 各トークンのIDを表すテンソル\n",
    "* **attention_mask**: パディング部分を除いたマスクを表すテンソル\n",
    "* **token_type_ids**: 文章の種類を表すテンソル (複数文章の場合は使用されない)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 BERTと単語ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = bert_model(**encoding, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, BERTモデルに `encoding` で表される入力データを入力し, モデルの出力を `output` 変数に代入しています. \n",
    "\n",
    "**1.1 `bert_model`**\n",
    "\n",
    "* `bert_model`: `BertModel` オブジェクト\n",
    "\n",
    "この部分は, `bert_model` オブジェクトを使って, BERTモデルに処理を実行させます. \n",
    "\n",
    "**1.2 `**encoding`**`\n",
    "\n",
    "* `encoding`: 変換済みの入力データ\n",
    "\n",
    "この部分は, `encoding` 変数に格納されている, `tokenizer` で変換済みの入力データを渡します. \n",
    "\n",
    "**1.3 `output_hidden_states=True`**\n",
    "\n",
    "* `output_hidden_states`: 隠れ層の出力を出力するかどうか\n",
    "\n",
    "このオプションは, BERTモデルの各層の隠れ層の出力を出力するかどうかを指定します. `output_hidden_states=True` を指定すると, 各層の隠れ層の出力が `output` 変数に格納されます. \n",
    "\n",
    "**2. `output` 変数の内容**\n",
    "\n",
    "`output` 変数には, 以下の情報が格納されたテンソルが入ります. \n",
    "\n",
    "* **last_hidden_state**: 最終層の隠れ層の出力を表すテンソル\n",
    "* **hidden_states**: 各層の隠れ層の出力を表すリスト\n",
    "* **pooler_output**: プーリング層の出力を表すテンソル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[2])\n",
    "# output[2][層][文][単語]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, BERTモデルの出力を分析するためのものです. \n",
    "\n",
    "**1.1 `len(output[2])`**\n",
    "\n",
    "この行は, `output[2]` の長さを出力します. \n",
    "\n",
    "* `output`: BERTモデルの出力を表す変数\n",
    "* `2`: `output` の3番目の要素\n",
    "\n",
    "`output[2]` は, 各層の隠れ層の出力を表すリストです. よって, この行は, BERTモデルが持つ隠れ層の数を出力することになります. \n",
    "\n",
    "**1.2 `output[2][層][文][単語]`**\n",
    "\n",
    "この行は, `output[2]` の要素にアクセスし, さらにその要素にアクセスして, 特定の隠れ層, 文, 単語に対応する値を取得します. \n",
    "\n",
    "* `output[2]`: 各層の隠れ層の出力を表すリスト\n",
    "* `層`: アクセスしたい隠れ層の番号\n",
    "* `文`: アクセスしたい文の番号\n",
    "* `単語`: アクセスしたい単語の番号\n",
    "\n",
    "例えば, `output[2][1][0][0]` は, 2番目の隠れ層, 1番目の文, 0番目の単語に対応する値を取得します. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm reading a book\",\n",
       " \"I'd like to book a room\",\n",
       " \"I'd like to reserve a room\",\n",
       " 'Please reserve this table for us']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm reading a book , I'd like to book a room\n",
      "0層目: 1.0\n",
      "最終層目: 0.359\n"
     ]
    }
   ],
   "source": [
    "# 多義語\n",
    "print(text_list[0], \",\", text_list[1])\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][0][6].reshape(1, -1), output[2][0][1][6].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][0][6].reshape(1, -1), output[2][-1][1][6].reshape(1, -1))\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `text_list` の最初の2つの英文 \"I'm reading a book\" と \"I'd like to book a room\" に対し, BERTモデルを用いて多義語表現の層間類似度を分析しています. \n",
    "\n",
    "\n",
    "**1.1 対象英文の表示**\n",
    "\n",
    "```python\n",
    "print(text_list[0], \",\", text_list[1])\n",
    "```\n",
    "\n",
    "この行は, `text_list` の最初の2つの英文 \"I'm reading a book\" と \"I'd like to book a room\" を出力します. これらの英文は, \"book\" という単語を含むものの, その意味は異なります. \n",
    "\n",
    "* **1番目の英文**: \"I'm reading a book\" - 本を読んでいます. \n",
    "* **2番目の英文**: \"I'd like to book a room\" - 部屋を予約したいです. \n",
    "\n",
    "**1.2 層間類似度の計算**\n",
    "\n",
    "```python\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][0][6].reshape(1, -1), output[2][0][1][6].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][0][6].reshape(1, -1), output[2][-1][1][6].reshape(1, -1))\n",
    "```\n",
    "\n",
    "この部分は, 以下の2つのステップでBERTモデルの層間類似度を計算しています. \n",
    "\n",
    "**ステップ1: 特定単語の隠れ層ベクトルの取得**\n",
    "\n",
    "* `output[2]`: 各層の隠れ層の出力を表すリスト\n",
    "* `0`, `-1`: アクセスする隠れ層の番号 (0層目と最終層目)\n",
    "* `0`, `1`: アクセスする文の番号 (最初の2つの文)\n",
    "* `6`: アクセスする単語の番号 (\"book\" に対応する単語)\n",
    "\n",
    "上記のインデックスを使って, \"book\" という単語に対応する各層の隠れ層ベクトルを取得します. \n",
    "\n",
    "**ステップ2: コサイン類似度の計算**\n",
    "\n",
    "取得した隠れ層ベクトル同士の**コサイン類似度** を計算します. コサイン類似度とは, 2つのベクトルの間の角度を cosθ で表し, 0に近いほどベクトルが似ていることを示す指標です. \n",
    "\n",
    "このコードでは, `torch.nn.functional.cosine_similarity` 関数を使ってコサイン類似度を計算しています. \n",
    "\n",
    "**1.3 類似度の出力**\n",
    "\n",
    "```python\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")\n",
    "```\n",
    "\n",
    "この部分は, 計算されたコサイン類似度を出力します. \n",
    "\n",
    "* **0層目**: 0番目の隠れ層における類似度\n",
    "* **最終層目**: 最終層目 (12番目の隠れ層) における類似度\n",
    "\n",
    "**2. コードの解釈**\n",
    "\n",
    "このコードは, BERTモデルの異なる層における\"book\" という単語の表現を比較し, 多義語表現の変化を分析しています. \n",
    "\n",
    "* **0層目**: 入力に近い情報を保持する層です. この層での類似度が高い場合, \"book\" という単語の基本的な意味が類似していることを示唆します. \n",
    "* **最終層目**: 文脈の影響を強く受けた情報を保持する層です. この層での類似度が高い場合, \"book\" という単語が文脈によってどのように解釈されているかが似ていることを示唆します. \n",
    "\n",
    "**3. 実行結果の解釈**\n",
    "\n",
    "出力された類似度の値を解釈するには, 以下の点に注意する必要があります. \n",
    "\n",
    "* **コサイン類似度の値の範囲**: 0に近いほどベクトルが似ており, 1に近いほどベクトルが異なっています. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to book a room , I'd like to reserve a room\n",
      "0層目: 0.096\n",
      "最終層目: 0.717\n"
     ]
    }
   ],
   "source": [
    "# 類義語(同じ文脈)\n",
    "print(text_list[1], \",\", text_list[2])\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][1][6].reshape(1, -1), output[2][0][2][6].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][1][6].reshape(1, -1), output[2][-1][2][6].reshape(1, -1))\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `text_list` の2番目の2つの英文 \"I'd like to book a room\" と \"I'd like to reserve a room\" に対し, BERTモデルを用いて**同じ文脈における類義語表現**の層間類似度を分析しています. \n",
    "\n",
    "**1. コードの詳細**\n",
    "\n",
    "```python\n",
    "# 類義語(同じ文脈)\n",
    "print(text_list[1], \",\", text_list[2])\n",
    "\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][1][6].reshape(1, -1), output[2][0][2][6].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][1][6].reshape(1, -1), output[2][-1][2][6].reshape(1, -1))\n",
    "\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")\n",
    "```\n",
    "\n",
    "**1.1 対象英文の表示**\n",
    "\n",
    "```python\n",
    "print(text_list[1], \",\", text_list[2])\n",
    "```\n",
    "\n",
    "この行は, `text_list` の2番目の2つの英文 \"I'd like to book a room\" と \"I'd like to reserve a room\" を出力します. これらの英文は, \"book\" と \"reserve\" という類義語を含むものの, 文脈は同じです. \n",
    "\n",
    "* **1番目の英文**: \"I'd like to book a room\" - 部屋を予約したいです. \n",
    "* **2番目の英文**: \"I'd like to reserve a room\" - 部屋を予約したいです. \n",
    "\n",
    "**1.2 層間類似度の計算**\n",
    "\n",
    "```python\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][1][6].reshape(1, -1), output[2][0][2][6].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][1][6].reshape(1, -1), output[2][-1][2][6].reshape(1, -1))\n",
    "```\n",
    "\n",
    "この部分は, 以下の2つのステップでBERTモデルの層間類似度を計算しています. \n",
    "\n",
    "**ステップ1: 特定単語の隠れ層ベクトルの取得**\n",
    "\n",
    "* `output[2]`: 各層の隠れ層の出力を表すリスト\n",
    "* `0`, `-1`: アクセスする隠れ層の番号 (0層目と最終層目)\n",
    "* `1`, `2`: アクセスする文の番号 (2番目の2つの文)\n",
    "* `6`: アクセスする単語の番号 (\"book\" または \"reserve\" に対応する単語)\n",
    "\n",
    "上記のインデックスを使って, \"book\" と \"reserve\" という単語に対応する各層の隠れ層ベクトルを取得します. \n",
    "\n",
    "**ステップ2: コサイン類似度の計算**\n",
    "\n",
    "取得した隠れ層ベクトル同士の**コサイン類似度** を計算します. \n",
    "\n",
    "**1.3 類似度の出力**\n",
    "\n",
    "```python\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")\n",
    "```\n",
    "\n",
    "この部分は, 計算されたコサイン類似度を出力します. \n",
    "\n",
    "* **0層目**: 0番目の隠れ層における類似度\n",
    "* **最終層目**: 最終層目 (12番目の隠れ層) における類似度\n",
    "\n",
    "**2. コードの解釈**\n",
    "\n",
    "このコードは, BERTモデルの異なる層における\"book\" と \"reserve\" という類義語の表現を比較し, **同じ文脈における類義語表現の変化**を分析しています. \n",
    "\n",
    "* **0層目**: 入力に近い情報を保持する層です. この層での類似度が高い場合, \"book\" と \"reserve\" という単語の基本的な意味が類似していることを示唆します. \n",
    "* **最終層目**: 文脈の影響を強く受けた情報を保持する層です. この層での類似度が高い場合, \"book\" と \"reserve\" という単語が文脈によってどのように解釈されているかが似ていることを示唆します. \n",
    "\n",
    "**3. 実行結果の解釈**\n",
    "\n",
    "出力された類似度の値を解釈するには, 以下の点に注意する必要があります. \n",
    "\n",
    "* **コサイン類似度の値の範囲**: 0に近いほどベクトルが似ており, 1に近いほどベ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to book a room , Please reserve this table for us\n",
      "0層目: 0.026\n",
      "最終層目: 0.415\n"
     ]
    }
   ],
   "source": [
    "# 類義語(違う文脈)\n",
    "print(text_list[1], \",\", text_list[3])\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][1][6].reshape(1, -1), output[2][0][3][2].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][1][6].reshape(1, -1), output[2][-1][3][2].reshape(1, -1))\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `text_list` の2番目の2つの英文 \"I'd like to book a room\" と \"Please reserve this table for us\" に対し, BERTモデルを用いて**異なる文脈における類義語表現**の層間類似度を分析しています. \n",
    "\n",
    "**1. コードの詳細**\n",
    "\n",
    "```python\n",
    "# 類義語(違う文脈)\n",
    "print(text_list[1], \",\", text_list[3])\n",
    "\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][1][6].reshape(1, -1), output[2][0][3][2].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][1][6].reshape(1, -1), output[2][-1][3][2].reshape(1, -1))\n",
    "\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")\n",
    "```\n",
    "\n",
    "**1.1 対象英文の表示**\n",
    "\n",
    "```python\n",
    "print(text_list[1], \",\", text_list[3])\n",
    "```\n",
    "\n",
    "この行は, `text_list` の2番目の2つの英文 \"I'd like to book a room\" と \"Please reserve this table for us\" を出力します. これらの英文は, \"book\" と \"reserve\" という類義語を含むものの, 文脈は異なります. \n",
    "\n",
    "* **1番目の英文**: \"I'd like to book a room\" - 部屋を予約したいです. \n",
    "* **2番目の英文**: \"Please reserve this table for us\" - このテーブルを私たちのために予約してください. \n",
    "\n",
    "**1.2 層間類似度の計算**\n",
    "\n",
    "```python\n",
    "layer0_similarity = F.cosine_similarity(output[2][0][1][6].reshape(1, -1), output[2][0][3][2].reshape(1, -1))\n",
    "layer12_similarity = F.cosine_similarity(output[2][-1][1][6].reshape(1, -1), output[2][-1][3][2].reshape(1, -1))\n",
    "```\n",
    "\n",
    "この部分は, 以下の2つのステップでBERTモデルの層間類似度を計算しています. \n",
    "\n",
    "**ステップ1: 特定単語の隠れ層ベクトルの取得**\n",
    "\n",
    "* `output[2]`: 各層の隠れ層の出力を表すリスト\n",
    "* `0`, `-1`: アクセスする隠れ層の番号 (0層目と最終層目)\n",
    "* `1`, `3`: アクセスする文の番号 (2番目の2つの文)\n",
    "* `6`, `2`: アクセスする単語の番号 (\"book\" または \"reserve\" に対応する単語)\n",
    "\n",
    "上記のインデックスを使って, \"book\" と \"reserve\" という単語に対応する各層の隠れ層ベクトルを取得します. \n",
    "\n",
    "**ステップ2: コサイン類似度の計算**\n",
    "\n",
    "取得した隠れ層ベクトル同士の**コサイン類似度** を計算します. \n",
    "\n",
    "**1.3 類似度の出力**\n",
    "\n",
    "```python\n",
    "print(f\"0層目: {round(layer0_similarity.item(), 3)}\")\n",
    "print(f\"最終層目: {round(layer12_similarity.item(), 3)}\")\n",
    "```\n",
    "\n",
    "この部分は, 計算されたコサイン類似度を出力します. \n",
    "\n",
    "* **0層目**: 0番目の隠れ層における類似度\n",
    "* **最終層目**: 最終層目 (12番目の隠れ層) における類似度\n",
    "\n",
    "**2. コードの解釈**\n",
    "\n",
    "このコードは, BERTモデルの異なる層における\"book\" と \"reserve\" という類義語の表現を比較し, **異なる文脈における類義語表現の変化**を分析しています. \n",
    "\n",
    "* **0層目**: 入力に近い情報を保持する層です. この層での類似度が高い場合, \"book\" と \"reserve\" という単語の基本的な意味が類似していることを示唆します. \n",
    "* **最終層目**: 文脈の影響を強く受けた情報を保持する層です. この層での類似度が高い場合, \"book\" と \"reserve\" という単語が文脈によってどのように解釈されているかが似ていることを示唆します. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
