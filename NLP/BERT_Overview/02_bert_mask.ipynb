{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 モデルの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (4.42.3)\n",
      "Requirement already satisfied: fugashi in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: ipadic in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: filelock in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers fugashi ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, 自然言語処理ライブラリである `transformers` を使って, 日本語 BERT モデルをロードするための準備をしています. \n",
    "\n",
    "**1.1 `import torch`**\n",
    "\n",
    "この行は, `torch` ライブラリをインポートします. `torch` ライブラリは, テンソルと呼ばれる多次元データ構造と, テンソル操作のための関数を提供するPythonライブラリです. BERTモデルは, `torch` ライブラリを使用して実装されています. \n",
    "\n",
    "**1.2 `from transformers import BertJapaneseTokenizer, BertForMaskedLM`**\n",
    "\n",
    "この行は, `transformers` ライブラリから `BertJapaneseTokenizer` と `BertForMaskedLM` クラスをインポートします. \n",
    "\n",
    "* **`BertJapaneseTokenizer`**: 日本語のテキストをトークン化するためのクラスです. \n",
    "* **`BertForMaskedLM`**: マスクされた言語モデリングタスク用の日本語 BERT モデルのクラスです. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリを使用して, 以下の2つの処理を実行しています. \n",
    "\n",
    "1. **日本語 BERT モデルのロード**: 事前に学習済み済みの日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" をロードします. \n",
    "2. **モデルの準備**: ロードしたモデルを, マスクされた言語モデリングタスクを実行できるように準備します. \n",
    "\n",
    "**1.1 `model_name`**\n",
    "\n",
    "この行は, 使用する日本語 BERT モデルの名前を `\"cl-tohoku/bert-base-japanese-whole-word-masking\"` に設定します. このモデルは, 東北大学で開発された日本語 BERT モデルであり, 日本語の単語全体をマスクして学習されています. \n",
    "\n",
    "**1.2 `tokenizer`**\n",
    "\n",
    "この行は, `BertJapaneseTokenizer` クラスを使用して, `model_name` で指定されたモデルに対応するトークナイザをロードします. トークナイザは, 日本語のテキストを BERT モデルが入力として理解できる形式に変換するために必要な処理を行います. \n",
    "\n",
    "**1.3 `bert_model`**\n",
    "\n",
    "この行は, `BertForMaskedLM` クラスを使用して, `model_name` で指定されたモデルに対応する BERT モデルをロードします. BERT モデルは, マスクされた言語モデリングタスクを実行するために必要な処理を行います. \n",
    "\n",
    "**2. まとめ**\n",
    "このコードは, `transformers` ライブラリを使用して, 日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" をロードし, マスクされた言語モデリングタスクを実行できるように準備します. 日本語のテキスト処理や, マスクされた言語モデリングなどのタスクに利用できます. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bert_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=32000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['私', 'は', 'お', '##腹', 'が', '空い', 'た', 'ので', '[MASK]', 'を', '食べ', 'たい']\n"
     ]
    }
   ],
   "source": [
    "text = \"私はお腹が空いたので[MASK]を食べたい\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 以下の処理を実行しています. \n",
    "\n",
    "1. **日本語文の入力**: \"私はお腹が空いたので[MASK]を食べたい\" という日本語文を `text` 変数に格納します. \n",
    "2. **トークナイゼーション**: `tokenizer.tokenize()` 関数を使って, `text` 変数に格納された日本語文をトークンに分割します. \n",
    "3. **トークンの出力**: 分割されたトークンを `print` 関数を使って出力します. \n",
    "\n",
    "\n",
    "**1.1 `text`**\n",
    "\n",
    "この行は, \"私はお腹が空いたので[MASK]を食べたい\" という日本語文を `text` 変数に格納します. \n",
    "\n",
    "**1.2 `tokens = tokenizer.tokenize(text)`**\n",
    "\n",
    "この行は, `tokenizer.tokenize()` 関数を使って, `text` 変数に格納された日本語文をトークンに分割します. `tokenizer.tokenize()` 関数は, 以下の処理を行います. \n",
    "\n",
    "* 空白や句読点などの記号を考慮して, 日本語文を単語に分割します. \n",
    "* 日本語 BERT モデルのボキャブラリに登録されている単語に対して, その単語に対応するIDを割り当てます. \n",
    "* マスクされた単語の位置に, 特別なIDを割り当てます. \n",
    "\n",
    "**1.3 `print(tokens)`**\n",
    "\n",
    "この行は, `print` 関数を使って, `tokens` 変数に格納されたトークンを出力します. \n",
    "\n",
    "**2. コードの出力例**\n",
    "\n",
    "```\n",
    "['私は', 'お腹', 'が', '空', 'いた', 'ので', '[MASK]', 'を', '食べ', 'たい']\n",
    "```\n",
    "\n",
    "上記の出力例では, \"私はお腹が空いたので[MASK]を食べたい\" という日本語文が, 以下の10個のトークンに分割されています. \n",
    "\n",
    "* **私は**: 格助詞\n",
    "* **お腹**: 名詞\n",
    "* **が**: 格助詞\n",
    "* **空**: 形容詞\n",
    "* **いた**: 動詞\n",
    "* **ので**: 接続詞\n",
    "* **[MASK]**: マスクされた単語\n",
    "* **を**: 格助詞\n",
    "* **食べ**: 動詞\n",
    "* **たい**: 動詞\n",
    "\n",
    "**3. まとめ**\n",
    "\n",
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 日本語文をトークンに分割する方法を示しました. マスクされた言語モデリングなどのタスクを実行する際には, このトークン化処理が必要となります. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(text, max_length=20, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 以下の処理を実行しています. \n",
    "\n",
    "1. **トークン化**: `tokenizer.tokenize()` 関数を使って, `text` 変数に格納された日本語文をトークンに分割します. \n",
    "2. **パディング**: 分割されたトークンに対して, `max_length` で指定された長さになるようにパディング処理を行います. \n",
    "3. **テンソル化**: パディング処理されたトークンを, `torch` ライブラリのテンソルに変換します. \n",
    "4. **出力**: 変換されたテンソルを `encoding` 変数に格納します. \n",
    "\n",
    "**1.1 `tokenizer(text, ...)`**\n",
    "\n",
    "この部分は, `tokenizer.tokenize()` 関数と同じ処理を実行します. 詳細は, 前の回答（[https://m.youtube.com/watch?v=h_U27jBNYI4](https://m.youtube.com/watch?v=h_U27jBNYI4)）を参照してください. \n",
    "\n",
    "**1.2 `max_length=20`**\n",
    "\n",
    "この引数は, パディング処理で使用する最大トークン長を20に設定します. つまり, 入力されたトークン数が20個以下であれば, そのままパディング処理されます. 20個を超える場合は, 長いトークンが切り捨てられます. \n",
    "\n",
    "**1.3 `padding=\"max_length\"`**\n",
    "\n",
    "この引数は, パディング処理の方法を `\"max_length\"` に設定します. `\"max_length\"` を選択すると, 以下の処理が行われます. \n",
    "\n",
    "* 入力されたトークンよりも少ない長さの場合は, 後ろに特別なパディングトークンを追加します. \n",
    "* 入力されたトークンが20個以下の場合は, パディング処理を行いません. \n",
    "\n",
    "**1.4 `truncation=True`**\n",
    "\n",
    "この引数は, トークン数が `max_length` を超えた場合に, 長いトークンを **先頭から** 切り捨てるかどうかを指定します. `True` に設定すると, 長いトークンが切り捨てられます. \n",
    "\n",
    "**1.5 `return_tensors=\"pt\"`**\n",
    "\n",
    "この引数は, 出力形式を `torch` ライブラリのテンソルに設定します. \n",
    "\n",
    "\n",
    "\n",
    "* **`input_ids`**: 各トークンに対応するIDを格納したテンソルです. \n",
    "* **`attention_mask`**: 各トークンが有効かどうかを格納したテンソルです. パディングされた部分は0, そうでない部分は1になります. \n",
    "\n",
    "**3. まとめ**\n",
    "\n",
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 日本語 BERT モデルに入力するためのデータ準備方法を示しました. マスクされた言語モデリングなどのタスクを実行する際には, このデータ準備処理が必要となります. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 BERTと単語ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 32000])\n"
     ]
    }
   ],
   "source": [
    "output = bert_model(**encoding)\n",
    "print(output[0].shape)\n",
    "mask_index = encoding[\"input_ids\"][0].tolist().index(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 以下の処理を実行しています. \n",
    "\n",
    "1. **BERTモデルへの入力**: `bert_model(**encoding)` 関数を使って, `encoding` 変数に格納されたテンソルを, BERTモデルに入力します. \n",
    "2. **モデルの出力**: BERTモデルから出力されたテンソルを `output` 変数に格納します. \n",
    "3. **マスクされた単語のインデックス取得**: `encoding[\"input_ids\"][0].tolist().index(4)` 関数を使って, マスクされた単語に対応するインデックスを `mask_index` 変数に格納します. \n",
    "4. **出力形状の表示**: `print(output[0].shape)` 関数を使って, `output[0]` テンソルの形状を出力します. \n",
    "\n",
    "**1.1 `output = bert_model(**encoding)`**\n",
    "\n",
    "この行は, `bert_model` 変数に格納された BERT モデルに対して, `encoding` 変数に格納されたテンソルを入力し, その出力を `output` 変数に格納します. \n",
    "\n",
    "**1.2 `print(output[0].shape)`**\n",
    "\n",
    "この行は, `output` 変数に格納されたテンソルの形状を出力します. BERTモデルの出力は, 通常複数のテンソルで構成されますが, このコードでは `output[0]` のみを出力しています. \n",
    "\n",
    "**1.3 `mask_index = encoding[\"input_ids\"][0].tolist().index(4)`**\n",
    "\n",
    "この行は, 以下の処理を実行します. \n",
    "\n",
    "* `encoding[\"input_ids\"][0]`: 入力されたトークンIDのテンソルを取得します. \n",
    "* `.tolist()`: テンソルをリストに変換します. \n",
    "* `.index(4)`: リストの中で, 4という値を持つ要素のインデックスを取得します. \n",
    "\n",
    "この処理により, `mask_index` 変数には, マスクされた単語に対応するインデックスが格納されます. \n",
    "\n",
    "**3. まとめ**\n",
    "\n",
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, BERTモデルの予測結果と, マスクされた単語のインデックスを取得する方法を示しました. マスクされた言語モデリングなどのタスクを実行する際には, この処理が必要となります. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はお腹が空いたのでご飯を食べたい\n"
     ]
    }
   ],
   "source": [
    "max_word = output[0][0][mask_index].argmax().item()\n",
    "mask_word = tokenizer.convert_ids_to_tokens(max_word)\n",
    "print(text.replace(\"[MASK]\", mask_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 以下の処理を実行しています. \n",
    "\n",
    "1. **マスクされた単語の予測**: BERTモデルの出力から, マスクされた単語の予測結果を取得します. \n",
    "2. **予測単語の取得**: 予測結果に基づいて, 最も可能性の高い単語を `mask_word` 変数に格納します. \n",
    "3. **元の文への置換**: 入力文中のマスクされた部分を, 予測された単語で置換した新しい文を出力します. \n",
    "\n",
    "**1.1 `max_word = output[0][0][mask_index].argmax().item()`**\n",
    "\n",
    "この行は, 以下の処理を実行します. \n",
    "\n",
    "* `output[0][0][mask_index]`: BERTモデルの出力テンソルから, マスクされた単語に対応する確率分布を取得します. \n",
    "* `.argmax()`: 確率分布の中で, 最も高い値を持つ要素のインデックスを取得します. \n",
    "* `.item()`: テンソル要素をPythonの値に変換します. \n",
    "\n",
    "この処理により, `max_word` 変数には, マスクされた単語として最も可能性の高い単語に対応するIDが格納されます. \n",
    "\n",
    "**1.2 `mask_word = tokenizer.convert_ids_to_tokens(max_word)`**\n",
    "\n",
    "この行は, `tokenizer.convert_ids_to_tokens()` 関数を使って, `max_word` 変数に格納されたIDを, 実際の単語に変換します. \n",
    "\n",
    "**1.3 `print(text.replace(\"[MASK]\", mask_word))`**\n",
    "\n",
    "この行は, `text` 変数に格納された元の文中の \"[MASK]\" 部分を, `mask_word` 変数に格納された予測単語に置き換えた新しい文を出力します. \n",
    "\n",
    "**3. まとめ**\n",
    "\n",
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, マスクされた単語を予測し, 元の文に置換する方法を示しました. マスクされた言語モデリングなどのタスクを実行する際には, この処理が必要となります. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はお腹が空いたのでご飯を食べたい\n",
      "私はお腹が空いたので野菜を食べたい\n",
      "私はお腹が空いたので肉を食べたい\n",
      "私はお腹が空いたのでカレーを食べたい\n",
      "私はお腹が空いたのでラーメンを食べたい\n",
      "私はお腹が空いたので[UNK]を食べたい\n",
      "私はお腹が空いたのでパンを食べたい\n",
      "私はお腹が空いたので米を食べたい\n",
      "私はお腹が空いたので魚を食べたい\n",
      "私はお腹が空いたので飯を食べたい\n"
     ]
    }
   ],
   "source": [
    "top_words = output[0][0][mask_index].topk(10).indices\n",
    "for word_id in top_words:\n",
    "  word = tokenizer.convert_ids_to_tokens(word_id.item())\n",
    "  print(text.replace(\"[MASK]\", word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示されたコードスニペットは, 事前学習済み言語モデルを使用して, 文中のマスクされた単語「[UNK]」に代わるさまざまな候補を生成しています. モデルは, 空腹を感じている人が食べる可能性のあるさまざまな食品を候補として提案しています. \n",
    "\n",
    "1. ご飯 (Gohan): 米飯\n",
    "2. 野菜 (Yasai): 野菜\n",
    "3. 肉 (Niku): 肉類\n",
    "4. カレー (Kare): カレーライス\n",
    "5. ラーメン (Rāmen): ラーメン\n",
    "6. パン (Pan): パン\n",
    "7. 米 (Kome): 米飯\n",
    "8. 魚 (Sakana): 魚\n",
    "9. 飯 (I): 米飯\n",
    "\n",
    "しかし, モデルは \"[UNK]\" も候補の一つとして生成しています. これは, モデルがマスクされた位置の一番可能性の高い単語を自信を持って特定できないことを示唆しています. これは, 文脈が限られていることや, 「お腹が空いた」というフレーズの曖昧性による可能性があります. \n",
    "\n",
    "この場合, 「[UNK]」は, 話者の好みや状況に応じて, 話者が望むさまざまな食品を表すことができます. また, 話者が欲している特定の飲み物や軽食など, 食品以外のものを指すこともあります. \n",
    "\n",
    "この文が使用された文脈や話者の意図に関する追加情報がない限り, 「[UNK]」の意味を明確に判断することはできません. しかし, 提示された候補リストは, 話者の意図に合致する可能性のあるさまざまな可能性を提供しています. \n",
    "\n",
    "**追加情報があれば**\n",
    "\n",
    "この文が使用された文脈や話者の意図に関する情報があれば, [UNK] の意味についてより具体的な回答を提供できる可能性があります. 例えば, 以下のような情報があると役立ちます. \n",
    "\n",
    "* この文は誰によって, 誰に対して言われたのですか？\n",
    "* この文が言われた状況はどのようなものでしたか？\n",
    "* 話者は普段どのような食べ物を好みますか？\n",
    "\n",
    "これらの情報があれば, [UNK] の意味をより絞り込むことができます. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, 以下の処理を実行しています. \n",
    "\n",
    "1. **上位5つのマスク単語候補の取得**: BERTモデルの出力から, マスクされた単語の**上位5つの候補**を取得します. \n",
    "2. **候補単語の表示**: 各候補単語について, 元の文に置換した文を出力します. \n",
    "\n",
    "**1.1 `top_words = output[0][0][mask_index].topk(5).indices`**\n",
    "\n",
    "この行は, 以下の処理を実行します. \n",
    "\n",
    "* `output[0][0][mask_index]`: BERTモデルの出力テンソルから, マスクされた単語に対応する確率分布を取得します. \n",
    "* `.topk(5)`: 確率分布の中で, **上位5つ**の要素とそのインデックスを取得します. \n",
    "* `.indices`: 取得したインデックスのみを抽出します. \n",
    "\n",
    "この処理により, `top_words` 変数には, マスクされた単語として最も可能性の高い5つの単語に対応するIDが格納されます. \n",
    "\n",
    "**1.2 `for word_id in top_words:`**\n",
    "\n",
    "このループは, `top_words` 変数に格納された各IDに対して処理を実行します. \n",
    "\n",
    "**1.3 `word = tokenizer.convert_ids_to_tokens(word_id.item())`**\n",
    "\n",
    "この行は, `tokenizer.convert_ids_to_tokens()` 関数を使って, `word_id` 変数に格納されたIDを, 実際の単語に変換します. \n",
    "\n",
    "**1.4 `print(text.replace(\"[MASK]\", word))`**\n",
    "\n",
    "この行は, `text` 変数に格納された元の文中の \"[MASK]\" 部分を, `word` 変数に格納された予測単語に置き換えた新しい文を出力します. \n",
    "\n",
    "**3. まとめ**\n",
    "\n",
    "このコードは, `transformers` ライブラリと日本語 BERT モデル \"cl-tohoku/bert-base-japanese-whole-word-masking\" を使って, マスクされた単語を予測し, 上位5つの候補を元の文に置換して表示する方法を示しました. マスクされた言語モデリングなどのタスクを実行する際には, この処理が必要となります. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
