{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 環境準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, Google Colab上でGoogleドライブをマウントするためのものです. \n",
    "\n",
    "1. **`google.colab` モジュールのインポート**: `google.colab` モジュールは, Google Colab上で動作するPythonプログラムに, Googleドライブへのアクセスを提供します. \n",
    "2. **`drive` オブジェクトの作成**: `drive.mount()` 関数を呼び出すことで, `drive` オブジェクトを作成します. このオブジェクトは, Googleドライブへのアクセスを管理するために使用されます. \n",
    "3. **マウントポイントの指定**: `drive.mount()` 関数には, マウントポイントとして使用するローカルディレクトリパスを指定する引数 `mount_point` があります. このコードでは, `/content/drive` ディレクトリをマウントポイントとして指定しています. \n",
    "\n",
    "**2. コードの実行方法**\n",
    "\n",
    "このコードを実行するには, 以下の手順が必要です. \n",
    "\n",
    "1. Google Colabを開きます. \n",
    "2. コードセルを作成し, 上記のコードを貼り付けます. \n",
    "3. コードセルを実行します. \n",
    "\n",
    "**3. コード実行後の状態**\n",
    "\n",
    "コードが正常に実行されると, Googleドライブが `/content/drive` ディレクトリにマウントされます. つまり, `/content/drive` ディレクトリにアクセスすることで, Googleドライブ上のファイルやフォルダにアクセスできるようになります. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (4.42.3)\n",
      "Requirement already satisfied: fugashi in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: ipadic in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: filelock in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: demoji in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: neologdn in /Users/master/.pyenv/versions/3.12.2/lib/python3.12/site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers fugashi ipadic\n",
    "!pip install demoji\n",
    "!pip install neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
    "!tar -zxvf ldcc-20140209.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 データの準備(jsonファイルの作成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import demoji\n",
    "import neologdn\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, Pythonで以下のライブラリをインポートしています. \n",
    "\n",
    "* **`glob`**: ファイルパスのパターンマッチングを行うライブラリです. \n",
    "* **`os`**: オペレーティングシステムとのインタラクションを提供するライブラリです. \n",
    "* **`json`**: JSON形式のデータの読み書きを行うライブラリです. \n",
    "* **`re`**: 正規表現処理を行うライブラリです. \n",
    "* **`demoji`**: 絵文字の処理を行うライブラリです. \n",
    "* **`neologdn`**: 現代日本語の表記ゆらぎの処理を行うライブラリです. \n",
    "* **`string`**: 文字列処理に関わる定数や関数を提供するライブラリです. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie-enter', 'it-life-hack', 'kaden-channel', 'topic-news', 'livedoor-homme', 'peachy', 'sports-watch', 'dokujo-tsushin', 'smax']\n"
     ]
    }
   ],
   "source": [
    "path = \"./text\"\n",
    "text_dir = os.listdir(path)\n",
    "category_list = [f for f in text_dir if os.path.isdir(os.path.join(path, f))]\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **対象ディレクトリの指定**: `/content/text` ディレクトリを `path` 変数に格納します. \n",
    "2. **ディレクトリ一覧の取得**: `os.listdir(path)` 関数を使って, `path` ディレクトリ内にあるすべてのファイルとディレクトリのリストを取得します. \n",
    "3. **サブディレクトリの抽出**: リストされた各要素に対して, `os.path.isdir()` 関数を使ってディレクトリかどうかを判定し, ディレクトリのみを `category_list` 変数に格納します. \n",
    "4. **サブディレクトリリストの出力**: `print(category_list)` 関数を使って, `category_list` 変数に格納されたサブディレクトリリストを出力します. \n",
    "\n",
    "**1. コードの詳細**\n",
    "\n",
    "**1.1 `path = \"/content/text\"`**\n",
    "\n",
    "この行は, 対象となるディレクトリパスを `/content/text` に設定します. \n",
    "\n",
    "**1.2 `text_dir = os.listdir(path)`**\n",
    "\n",
    "この行は, `os.listdir()` 関数を使って, `path` ディレクトリ内にあるすべてのファイルとディレクトリのリストを取得し, `text_dir` 変数に格納します. \n",
    "\n",
    "**1.3 `category_list = [f for f in text_dir if os.path.isdir(os.path.join(path, f))]`**\n",
    "\n",
    "この行は, 以下の処理を実行します. \n",
    "\n",
    "* `f for f in text_dir`: `text_dir` リスト内の各要素 `f` についてループ処理を行います. \n",
    "* `os.path.isdir(os.path.join(path, f))`: `f` を `path` と結合したパスがディレクトリかどうかを判定します. \n",
    "* `if`: 判定結果が `True` の場合のみ, `f` を `category_list` に追加します. \n",
    "\n",
    "この処理により, `category_list` 変数には, `path` ディレクトリ内のサブディレクトリのみが格納されます. \n",
    "\n",
    "**1.4 `print(category_list)`**\n",
    "\n",
    "この行は, `print()` 関数を使って, `category_list` 変数に格納されたサブディレクトリリストを出力します. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'category': 'movie-enter'}, {'id': 1, 'category': 'it-life-hack'}, {'id': 2, 'category': 'kaden-channel'}, {'id': 3, 'category': 'topic-news'}, {'id': 4, 'category': 'livedoor-homme'}, {'id': 5, 'category': 'peachy'}, {'id': 6, 'category': 'sports-watch'}, {'id': 7, 'category': 'dokujo-tsushin'}, {'id': 8, 'category': 'smax'}]\n"
     ]
    }
   ],
   "source": [
    "id_category_list = []\n",
    "for index, category in enumerate(category_list):\n",
    "    category_dict = {\"id\": index, \"category\": category}\n",
    "    id_category_list.append(category_dict)\n",
    "print(f\"id_category_list: {id_category_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, カテゴリとそれに対応するIDを格納した辞書のリストを作成します. \n",
    "\n",
    "**1. 空のリストを作成する**\n",
    "\n",
    "最初の行 `id_category_list = []` は, `id_category_list` という名前の空のリストを作成します. このリストは, カテゴリとIDのペアを格納するために使用されます. \n",
    "\n",
    "**2. カテゴリをループ処理する**\n",
    "\n",
    "2番目の行 `for index, category in enumerate(category_list)` は, `category_list` 内の各カテゴリをループ処理します. このループは, `category_list` の要素数だけ繰り返されます. \n",
    "\n",
    "- `index` 変数は, 現在のカテゴリの `category_list` 内のインデックスを保持します. インデックスは 0 から始まり, 各ループで 1 ずつ増えます. \n",
    "- `category` 変数は, 現在のループで処理されているカテゴリの実際の値を保持します. \n",
    "\n",
    "**3. カテゴリとIDを格納した辞書を作成する**\n",
    "\n",
    "3番目の行 `category_dict = {\"id\": index, \"category\": category}` は, `category_dict` という名前の辞書を作成します. この辞書には, 2つのキーと値のペアが含まれます. \n",
    "\n",
    "- `\"id\"` キーには, 現在の `index` の値が割り当てられます. これは, このカテゴリの一意なIDを表します. \n",
    "- `\"category\"` キーには, `category_list` から取得した現在のカテゴリ名または値が格納されます. \n",
    "\n",
    "**4. 辞書をリストに追加する**\n",
    "\n",
    "4番目の行 `id_category_list.append(category_dict)` は, 新しく作成された `category_dict` を `id_category_list` に追加します. つまり, ループの各反復で 1 つの辞書がリストに追加され, カテゴリとIDのペアのリストが効率的に構築されます. \n",
    "\n",
    "**5. 最終的なリストを出力する**\n",
    "\n",
    "最後の行 `print(id_category_list)` は, 単に `id_category_list` の内容をコンソールに出力します. これにより, 作成されたカテゴリとIDの辞書のリストを確認できます. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': 'movie-enter-6361791 3.txt', 'label': 0, 'category_name': 'movie-enter'}, {'file_name': 'movie-enter-5978741.txt', 'label': 0, 'category_name': 'movie-enter'}, {'file_name': 'movie-enter-6322901.txt', 'label': 0, 'category_name': 'movie-enter'}, {'file_name': 'movie-enter-6316535 2.txt', 'label': 0, 'category_name': 'movie-enter'}]\n"
     ]
    }
   ],
   "source": [
    "annotations_list = []\n",
    "for item in id_category_list:\n",
    "    file_list = glob.glob(f'text/{item[\"category\"]}/{item[\"category\"]}*.txt')\n",
    "    for file in file_list:\n",
    "        annotation_dict = {\"file_name\": os.path.basename(file), \"label\": item[\"id\"], \"category_name\": item[\"category\"]}\n",
    "        annotations_list.append(annotation_dict)\n",
    "print(annotations_list[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `annotations_list` という空のリストを作成します. \n",
    "* `id_category_list` というリストをループ処理します. \n",
    "* 各要素 `item` について, 以下の処理を実行します. \n",
    "    * `glob.glob()` 関数を使って, `text/{item[\"category\"]}/{item[\"category\"]}*.txt` というパターンに一致するファイルパスのリストを取得します. \n",
    "    * 取得したファイルパスリスト `file_list` をループ処理します. \n",
    "    * 各ファイルパス `file` について, 以下の情報を格納した辞書 `annotation_dict` を作成します. \n",
    "        * `file_name`: ファイル名\n",
    "        * `label`: ラベル (item[\"id\"])\n",
    "        * `category_name`: カテゴリ名 (item[\"category\"])\n",
    "    * 作成した `annotation_dict` を `annotations_list` に追加します. \n",
    "* `annotations_list[:4]` を出力します. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dict = {\"category\": id_category_list, \"annotations\": annotations_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. JSON形式の辞書の作成**\n",
    "\n",
    "* `json_dict` という空の辞書を作成します. \n",
    "* `json_dict` に以下のキーと値を格納します. \n",
    "    * `category`: `id_category_list` の内容\n",
    "    * `annotations`: `annotations_list` の内容\n",
    "* 作成した `json_dict` を返します. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_save_path = \"./text/dataset.json\"\n",
    "with open(json_save_path, mode=\"wt\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSONデータの保存**\n",
    "\n",
    "* `json_save_path` 変数に, 保存先のファイルパス `/content/text/dataset.json` を設定します. \n",
    "* `with open()` ステートメントを使って, `json_save_path` を `wt` モード (書き込みモード) と `utf-8` エンコーディングで開きます. \n",
    "* `json.dump()` 関数を使って, `json_dict` 変数に格納されたJSONデータをファイルに書き込みます. \n",
    "* 書き込み後, ファイルは自動的に閉じられます. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Dataloader作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ec0b930>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertJapaneseTokenizer\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, PyTorchとBERT Japanese Tokenizerを使って機械学習のタスクを実行するための準備を行うものです. \n",
    "\n",
    "**1. ライブラリのインポート**\n",
    "\n",
    "* `torch`: PyTorch本体のライブラリ\n",
    "* `torch.utils.data`: PyTorch DataLoaderなどのデータ処理モジュール\n",
    "* `Dataset`: PyTorch DataLoader用のデータセットクラス\n",
    "* `DataLoader`: PyTorch DataLoaderクラス\n",
    "* `BertJapaneseTokenizer`: BERT Japanese Tokenizer\n",
    "\n",
    "**1.1 `import torch`**\n",
    "\n",
    "この行は, PyTorch本体のライブラリをインポートします. 機械学習モデルの構築やデータ処理など, PyTorchの様々な機能を利用するために必要です. \n",
    "\n",
    "**1.2 `from torch.utils.data import Dataset, DataLoader`**\n",
    "\n",
    "この行は, PyTorch DataLoader用のデータセットクラス `Dataset` と DataLoaderクラス `DataLoader` をインポートします. これらのクラスは, 機械学習モデルの訓練や評価に使用するデータを効率的に処理するために使用されます. \n",
    "\n",
    "**1.3 `from transformers import BertJapaneseTokenizer`**\n",
    "\n",
    "この行は, BERT Japanese Tokenizerをインポートします. これは, 日本語テキストをBERTモデルで処理するために必要なトークン化を行うライブラリです. \n",
    "\n",
    "**1.4 `torch.manual_seed(0)`**\n",
    "\n",
    "この行は, PyTorchの乱数シードを0に設定します. これは, 機械学習モデルの訓練結果を再現できるようにするために重要です. 乱数シードを設定することで, 異なる環境でも同じ結果を得られるようにします. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LivedoorDataset(Dataset):\n",
    "    def __init__(self, tokenizer, text_dir=None):\n",
    "        self.text_dir = text_dir\n",
    "        self._load_json()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self._get_text(self.annotations_list[idx][\"category_name\"], \n",
    "                              self.annotations_list[idx][\"file_name\"])\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
    "        encoding = {key: torch.squeeze(value) for key, value in encoding.items()}\n",
    "        encoding[\"labels\"] = self.annotations_list[idx][\"label\"]\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def _load_json(self):\n",
    "        with open(os.path.join(self.text_dir, 'dataset.json')) as f:\n",
    "            self.text_json = json.load(f)\n",
    "        self.annotations_list = self.text_json[\"annotations\"]\n",
    "        \n",
    "    def _get_text(self, category_name, file_name):\n",
    "        file_path = os.path.join(self.text_dir, category_name, file_name)\n",
    "        lines = open(file_path).read().splitlines()\n",
    "        text = '\\n'.join(lines[3:]) # ファイルの4行目からを抜き出す. \n",
    "        text_preprocessed = self._text_preprocess(text)\n",
    "        return text_preprocessed\n",
    "        \n",
    "    def _text_preprocess(self, text):\n",
    "        # タブの消去\n",
    "        text = text.translate(str.maketrans({'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))\n",
    "\n",
    "        # URLの消去\n",
    "        text = re.sub(r'http?://[\\w/:%#$&\\?~\\.=\\+\\-]+', '', text)\n",
    "        text = re.sub(r'https?://[\\w/:%#$&\\?~\\.=\\+\\-]+', '', text)\n",
    "\n",
    "        # 絵文字の消去\n",
    "        text = demoji.replace(string=text, repl='')\n",
    "\n",
    "        # 文字の正規化\n",
    "        text = neologdn.normalize(text)\n",
    "\n",
    "        # 数字をすべて0に\n",
    "        text = re.sub(r'\\d+', '0', text)\n",
    "\n",
    "        # 大文字を小文字に\n",
    "        text = text.lower()\n",
    "\n",
    "        # 【関連記事, 関連サイト, 関連リンク】以降の消去\n",
    "        target_list = ['関連記事', '関連サイト', '関連リンク']\n",
    "        for target in target_list:\n",
    "          idx = text.find(target)\n",
    "          text = text[:(idx-1)]\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, PyTorch DataLoaders用の `LivedoorDataset` クラスを定義しています. このクラスは, Livedoorコーパスのテキストデータを読み込み, BERT Japanese Tokenizerを使ってトークン化し, PyTorchモデルで扱える形式に変換する役割を担います. \n",
    "\n",
    "**1. クラスの概要**\n",
    "\n",
    "`LivedoorDataset` クラスは, 以下の情報を持ちます. \n",
    "\n",
    "* `text_dir`: Livedoorコーパスデータのディレクトリパス\n",
    "* `tokenizer`: BERT Japanese Tokenizer\n",
    "* `annotations_list`: JSONファイルから読み込んだアノテーションデータ\n",
    "\n",
    "**2. メソッド解説**\n",
    "\n",
    "**2.1 `__init__()` メソッド**\n",
    "\n",
    "* コンストラクタ: LivedoorコーパスのディレクトリパスとBERT Japanese Tokenizerを受け取り, 以下の処理を実行します. \n",
    "    * `_load_json()` メソッドを呼び出して, JSONファイルからアノテーションデータを読み込みます. \n",
    "    * `self.tokenizer` に BERT Japanese Tokenizer を設定します. \n",
    "\n",
    "**2.2 `__len__()` メソッド**\n",
    "\n",
    "* データセットの長さを返します. \n",
    "    * `len(self.annotations_list)` を返します. \n",
    "\n",
    "**2.3 `__getitem__()` メソッド**\n",
    "\n",
    "* 指定されたインデックスのデータを取得します. \n",
    "    * `self.annotations_list[idx]` からアノテーションデータを取得します. \n",
    "    * `_get_text()` メソッドを使って, カテゴリ名とファイル名からテキストを取得します. \n",
    "    * BERT Japanese Tokenizerを使って, テキストをトークン化し, PyTorchモデルで扱える形式に変換します. \n",
    "    * 変換されたデータを辞書形式にして返します. \n",
    "\n",
    "**2.4 `_load_json()` メソッド**\n",
    "\n",
    "* JSONファイルからアノテーションデータを読み込みます. \n",
    "    * `os.path.join(self.text_dir, 'dataset.json')` のパスでJSONファイルを開きます. \n",
    "    * JSONデータを `self.text_json` に格納します. \n",
    "    * `self.annotations_list` に `self.text_json[\"annotations\"]` を設定します. \n",
    "\n",
    "**2.5 `_get_text()` メソッド**\n",
    "\n",
    "* カテゴリ名とファイル名からテキストを取得します. \n",
    "    * `os.path.join(self.text_dir, category_name, file_name)` のパスでファイルを開きます. \n",
    "    * ファイルの内容を `lines` に格納します. \n",
    "    * `lines[3:]` を使って, 4行目以降のテキストのみを取得します. \n",
    "    * `_text_preprocess()` メソッドを使って, テキストの前処理を行います. \n",
    "    * 前処理されたテキストを返します. \n",
    "\n",
    "**2.6 `_text_preprocess()` メソッド**\n",
    "\n",
    "* テキストの前処理を行います. \n",
    "    * タブ, URL, 絵文字, 文字の正規化, 数字の0化, 大文字の変換, 特定の文字列の削除などの処理を行います. \n",
    "    * 前処理されたテキストを返します. \n",
    "\n",
    "**3. まとめ**\n",
    "\n",
    "`LivedoorDataset` クラスは, Livedoorコーパスのテキストデータを読み込み, BERT Japanese Tokenizerを使ってトークン化し, PyTorchモデルで扱える形式に変換する機能を提供します. このクラスを利用することで, Livedoorコーパスデータを機械学習モデルで効率的に処理することができます. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26933\n",
      "7000 19933\n"
     ]
    }
   ],
   "source": [
    "dataset = LivedoorDataset(tokenizer, \"./text\")\n",
    "print(len(dataset))\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [7000, len(dataset)-7000])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, PyTorchを使ってLivedoorコーパスデータを読み込み, 訓練データセットと検証データセットを作成するものです. \n",
    "\n",
    "**1. LivedoorDatasetクラスの利用**\n",
    "\n",
    "* `dataset = LivedoorDataset(tokenizer, \"/content/text\")` の行では, LivedoorDatasetクラスを使って, Livedoorコーパスのテキストデータを読み込み, BERT Japanese Tokenizerを使ってトークン化し, PyTorchモデルで扱える形式に変換したデータセットを作成しています. \n",
    "* `/content/text` は, Livedoorコーパスのテキストデータが格納されているディレクトリパスです. \n",
    "* `tokenizer` は, BERT Japanese Tokenizerオブジェクトです. \n",
    "\n",
    "**2. データセットの長さの確認**\n",
    "\n",
    "* `print(len(dataset))` の行では, 作成したデータセットの長さを出力しています. \n",
    "* データセットの長さは, Livedoorコーパスに含まれるテキストデータの件数です. \n",
    "\n",
    "**3. 訓練・検証データセットの作成**\n",
    "\n",
    "* `train_dataset, val_dataset = torch.utils.data.random_split(dataset, [7000, len(dataset)-7000])` の行では, `random_split()` 関数を使って, 作成したデータセットをランダムに分割し, 訓練データセットと検証データセットを作成しています. \n",
    "* `[7000, len(dataset)-7000]` は, 分割する割合を指定します. この場合, データセット全体のうち7000件を訓練データセット, 残りの件数を検証データセットとしています. \n",
    "\n",
    "**4. DataLoaderの作成**\n",
    "\n",
    "* `train_dataloader = DataLoader(train_dataset, batch_size=8)` の行では, 訓練データセットを使って DataLoader を作成しています. \n",
    "* `batch_size` は, バッチサイズを指定します. この場合, 1つのバッチに8件のデータが含まれます. \n",
    "* `DataLoader` は, データをミニバッチに分割し, 訓練や検証時に効率的に処理するために使用されます. \n",
    "\n",
    "* `val_dataloader = DataLoader(val_dataset, batch_size=8)` の行では, 検証データセットを使って DataLoader を作成しています. \n",
    "* `batch_size` は, 訓練データセットと同じ8に設定されています. \n",
    "\n",
    "**5. 訓練・検証データセットの件数の確認**\n",
    "\n",
    "* `print(len(train_dataset), len(val_dataset))` の行では, 作成した訓練データセットと検証データセットの件数を出力しています. \n",
    "* 訓練データセットの件数は7000件, 検証データセットの件数は残りの件数となります. \n",
    "\n",
    "**6. まとめ**\n",
    "\n",
    "このコードは, LivedoorコーパスデータをPyTorchで読み込み, 訓練データセットと検証データセットを作成する手順を示しています. このデータセットを使用して, BERTモデルなどの機械学習モデルを訓練することができます. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "  device = torch.device('mps')\n",
    "else:\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=9).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2940bbb7a24973b4bc29b6e35d336d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7581e2ee344fc3874e61c157eee3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m   labels_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([labels_list, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()])\n\u001b[1;32m     28\u001b[0m   output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 29\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     30\u001b[0m   outputs_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([outputs_list, output])\n\u001b[1;32m     32\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(outputs_list \u001b[38;5;241m==\u001b[39m labels_list) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs_list) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "EPOCH = 1\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "\n",
    "  # Use tqdm for progress bar during training\n",
    "  for i, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "    batch = {key: value.to(device) for key, value in batch.items()}\n",
    "    output = model(**batch)\n",
    "    loss = output.loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss\n",
    "\n",
    "  model.eval()\n",
    "  labels_list, outputs_list = [], []\n",
    "\n",
    "  # Use tqdm for progress bar during validation\n",
    "  for i, batch in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "    batch = {key: value.to(device) for key, value in batch.items()}\n",
    "    labels_list = np.concatenate([labels_list, batch[\"labels\"].cpu().detach().numpy()])\n",
    "    output = model(**batch)\n",
    "    output = output.logits.argmax(axis=1).cpu().detach().numpy()\n",
    "    outputs_list = np.concatenate([outputs_list, output])\n",
    "\n",
    "  accuracy = sum(outputs_list == labels_list) / len(outputs_list) * 100\n",
    "  print(f\"epoch: {epoch + 1}, train_loss: {round(train_loss.item(), 1)}, accuracy: {round(accuracy, 1)}% {sum(outputs_list == labels_list)}/{len(outputs_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードは, PyTorchを使ってBERTモデルを訓練・評価するループ処理を示しています. \n",
    "\n",
    "**1. 訓練・評価ループ**\n",
    "\n",
    "* `EPOCH = 10` : 訓練を繰り返す回数 (エポック数) を設定します. \n",
    "* `for epoch in range(EPOCH)` : エポック数分のループを回します. \n",
    "\n",
    "**2. 訓練処理**\n",
    "\n",
    "* `model.train()` : モデルを訓練モードにします. \n",
    "* `train_loss = 0` : 訓練損失の累積変数を初期化します. \n",
    "* `for i, batch in enumerate(train_dataloader)` : 訓練データセットの DataLoader をイテレーションします. \n",
    "    * `batch` : ミニバッチのデータを辞書形式で取得します. \n",
    "    * `{key: value.to(device) for key, value in batch.items()}` : バッチ内の各データを指定したデバイス (CPU or GPU) に転送します. \n",
    "    * `output = model(**batch)` : モデルに入力データを流し, 出力を取得します. (** unpacking dictionary)\n",
    "    * `loss = output.loss` : 出力から損失値を取得します. \n",
    "    * `optimizer.zero_grad()` : 勾配をゼロにします. \n",
    "    * `loss.backward()` : 誤差逆伝播 (backpropagation) を実行します. \n",
    "    * `optimizer.step()` : パラメータを更新します. \n",
    "    * `train_loss += loss` : 訓練損失を累積します. \n",
    "\n",
    "**3. 評価処理**\n",
    "\n",
    "* `model.eval()` : モデルを評価モードにします. \n",
    "* `labels_list, outputs_list = [], []` : ラベルとモデルの予測結果を格納するリストを初期化します. \n",
    "* `for i, batch in enumerate(val_dataloader)` : 検証データセットの DataLoader をイテレーションします. \n",
    "    * 同様にバッチデータを取得し, デバイスに転送します. \n",
    "    * `labels_list` : 正解ラベルをリストに連結します. \n",
    "    * `output.logits.argmax(axis=1)` : モデルの出力からクラス確率 (logits) を取り出し, 最も確率の高いクラスインデックスを取得します.  (argmax)\n",
    "    * `outputs_list` : モデルの予測結果をリストに連結します. \n",
    "\n",
    "**4. 精度計算と出力**\n",
    "\n",
    "* `accuracy = ...` : 正解率 (accuracy) を計算します. \n",
    "* `print(f\"...\")` : エポック数, 訓練損失, 正解率, 分類ごとの正解数/総数を表示します. \n",
    "\n",
    "**5. まとめ**\n",
    "\n",
    "このコードは, BERTモデルを訓練・評価する処理をループで実行します. 各エポックでモデルを訓練し, 検証データセットで評価を行います. 訓練と評価の結果はコンソールに出力されます. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix(データについて)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.livedoor.com/article/detail/6617244/\n",
      "2012-06-04T08:00:00+0900\n",
      "ノオミ・ラパスかシャーリーズ・セロンか、『プロメテウス』のワールドプレミアでドレス対決\n",
      "　リドリー・スコット監督最新作『プロメテウス』のワールド・プレミアが、現地時間5月31日に、ロンドンのレスター・スクウェアにて開催された。\n",
      "\n",
      "　エリザベス女王陛下即位60周年記念に沸き、至る所に国旗が飾られているロンドン。中心街レスター・スクウェアに、鮮やかなブルーカーペットが敷き詰められて、ワールド・プレミアは行われた。今まで一切ストーリーが明かされず、その全貌が謎に包まれたままだった本作の初披露とあって、会場には約1,500名のファンが集結した。\n",
      "\n",
      "　『プロメテウス』は、人類史上最大の謎“人類の起源”を解き明かす重大なヒントを地球上の古代遺跡で発見し、宇宙船プロメテウス号に乗って未知の惑星を訪れた科学者チームの、想像を絶する運命を映し出す。めくるめく神秘と衝撃に彩られた探査航海の果てに、決して触れてはならない“パンドラの箱”が開いたとき、まだ何も知らない人類はすべてを目撃する。前人未踏の宇宙の彼方に、地球上のあらゆる歴史や文明の概念さえも覆す、驚愕の真実が眠っていたことを——。\n",
      "\n",
      "　出演は、『ミレニアム ドラゴン・タトゥーの女』で強烈な演技を見せたノオミ・ラパス、日本でも多くの女性たちから圧倒的な支持を受けるハリウッドを代表するオスカー女優シャーリーズ・セロン、『SHAME-シェイム-』でヴェネチア国際映画祭主演男優賞を獲得し、マグニート役で『X-MEN:ファースト・ジェネレーション』を崇高なドラマにまで昇華させた俳優マイケル・ファスベンダーらが集う。圧倒的なオリジナリティに満ちた先見性と、アーティスティックな映像感覚で名高いスコット監督は、いま最も多くの注目を集めている彼ら豪華布陣を配し、観る者の好奇心や空想をはるかに超越した“人類の起源”をビジュアル化した。 \n",
      "\n",
      "　主要キャストのノオミ・ラパス、シャーリーズ・セロン、マイケル・ファスベンダー、そして巨匠リドリー・スコット監督が会場に登場するとファンの熱気に包まれた。ノオミ・ラパスは胸元の空いた黒いドレス、シャーリーズ・セロンは金色の髪によく似合う青のドレスで集まった人々を魅了。写真撮影と数台のカメラの取材に応えた後、ファンサービスに終始していた。\n",
      "\n",
      "　映画『プロメテウス』は、8月24日（金）全国ロードショー＜3D/2D同時上映＞\n",
      "\n",
      "・映画『プロメテウス』オフィシャルサイト\n",
      "\n",
      "■関連記事\n",
      "・人類誕生の謎が明かされる！ 宇宙に飛び出した科学者が見たものとは\n",
      "・映画界の“生きる伝説”が人類の起源に迫る\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"text/movie-enter/movie-enter-6617244.txt\"\n",
    "txt_file = open(file_path).read()\n",
    "print(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://news.livedoor.com/article/detail/6617244/', '2012-06-04T08:00:00+0900', 'ノオミ・ラパスかシャーリーズ・セロンか、『プロメテウス』のワールドプレミアでドレス対決', '\\u3000リドリー・スコット監督最新作『プロメテウス』のワールド・プレミアが、現地時間5月31日に、ロンドンのレスター・スクウェアにて開催された。', '', '\\u3000エリザベス女王陛下即位60周年記念に沸き、至る所に国旗が飾られているロンドン。中心街レスター・スクウェアに、鮮やかなブルーカーペットが敷き詰められて、ワールド・プレミアは行われた。今まで一切ストーリーが明かされず、その全貌が謎に包まれたままだった本作の初披露とあって、会場には約1,500名のファンが集結した。', '', '\\u3000『プロメテウス』は、人類史上最大の謎“人類の起源”を解き明かす重大なヒントを地球上の古代遺跡で発見し、宇宙船プロメテウス号に乗って未知の惑星を訪れた科学者チームの、想像を絶する運命を映し出す。めくるめく神秘と衝撃に彩られた探査航海の果てに、決して触れてはならない“パンドラの箱”が開いたとき、まだ何も知らない人類はすべてを目撃する。前人未踏の宇宙の彼方に、地球上のあらゆる歴史や文明の概念さえも覆す、驚愕の真実が眠っていたことを——。', '', '\\u3000出演は、『ミレニアム ドラゴン・タトゥーの女』で強烈な演技を見せたノオミ・ラパス、日本でも多くの女性たちから圧倒的な支持を受けるハリウッドを代表するオスカー女優シャーリーズ・セロン、『SHAME-シェイム-』でヴェネチア国際映画祭主演男優賞を獲得し、マグニート役で『X-MEN:ファースト・ジェネレーション』を崇高なドラマにまで昇華させた俳優マイケル・ファスベンダーらが集う。圧倒的なオリジナリティに満ちた先見性と、アーティスティックな映像感覚で名高いスコット監督は、いま最も多くの注目を集めている彼ら豪華布陣を配し、観る者の好奇心や空想をはるかに超越した“人類の起源”をビジュアル化した。 ', '', '\\u3000主要キャストのノオミ・ラパス、シャーリーズ・セロン、マイケル・ファスベンダー、そして巨匠リドリー・スコット監督が会場に登場するとファンの熱気に包まれた。ノオミ・ラパスは胸元の空いた黒いドレス、シャーリーズ・セロンは金色の髪によく似合う青のドレスで集まった人々を魅了。写真撮影と数台のカメラの取材に応えた後、ファンサービスに終始していた。', '', '\\u3000映画『プロメテウス』は、8月24日（金）全国ロードショー＜3D/2D同時上映＞', '', '・映画『プロメテウス』オフィシャルサイト', '', '■関連記事', '・人類誕生の謎が明かされる！ 宇宙に飛び出した科学者が見たものとは', '・映画界の“生きる伝説”が人類の起源に迫る']\n"
     ]
    }
   ],
   "source": [
    "lines = txt_file.splitlines()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　リドリー・スコット監督最新作『プロメテウス』のワールド・プレミアが、現地時間5月31日に、ロンドンのレスター・スクウェアにて開催された。\n",
      "\n",
      "　エリザベス女王陛下即位60周年記念に沸き、至る所に国旗が飾られているロンドン。中心街レスター・スクウェアに、鮮やかなブルーカーペットが敷き詰められて、ワールド・プレミアは行われた。今まで一切ストーリーが明かされず、その全貌が謎に包まれたままだった本作の初披露とあって、会場には約1,500名のファンが集結した。\n",
      "\n",
      "　『プロメテウス』は、人類史上最大の謎“人類の起源”を解き明かす重大なヒントを地球上の古代遺跡で発見し、宇宙船プロメテウス号に乗って未知の惑星を訪れた科学者チームの、想像を絶する運命を映し出す。めくるめく神秘と衝撃に彩られた探査航海の果てに、決して触れてはならない“パンドラの箱”が開いたとき、まだ何も知らない人類はすべてを目撃する。前人未踏の宇宙の彼方に、地球上のあらゆる歴史や文明の概念さえも覆す、驚愕の真実が眠っていたことを——。\n",
      "\n",
      "　出演は、『ミレニアム ドラゴン・タトゥーの女』で強烈な演技を見せたノオミ・ラパス、日本でも多くの女性たちから圧倒的な支持を受けるハリウッドを代表するオスカー女優シャーリーズ・セロン、『SHAME-シェイム-』でヴェネチア国際映画祭主演男優賞を獲得し、マグニート役で『X-MEN:ファースト・ジェネレーション』を崇高なドラマにまで昇華させた俳優マイケル・ファスベンダーらが集う。圧倒的なオリジナリティに満ちた先見性と、アーティスティックな映像感覚で名高いスコット監督は、いま最も多くの注目を集めている彼ら豪華布陣を配し、観る者の好奇心や空想をはるかに超越した“人類の起源”をビジュアル化した。 \n",
      "\n",
      "　主要キャストのノオミ・ラパス、シャーリーズ・セロン、マイケル・ファスベンダー、そして巨匠リドリー・スコット監督が会場に登場するとファンの熱気に包まれた。ノオミ・ラパスは胸元の空いた黒いドレス、シャーリーズ・セロンは金色の髪によく似合う青のドレスで集まった人々を魅了。写真撮影と数台のカメラの取材に応えた後、ファンサービスに終始していた。\n",
      "\n",
      "　映画『プロメテウス』は、8月24日（金）全国ロードショー＜3D/2D同時上映＞\n",
      "\n",
      "・映画『プロメテウス』オフィシャルサイト\n",
      "\n",
      "■関連記事\n",
      "・人類誕生の謎が明かされる！ 宇宙に飛び出した科学者が見たものとは\n",
      "・映画界の“生きる伝説”が人類の起源に迫る\n"
     ]
    }
   ],
   "source": [
    "text = '\\n'.join(lines[3:]) # ファイルの4行目からを抜き出す. \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    # タブの消去\n",
    "    text = text.translate(str.maketrans({'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))\n",
    "\n",
    "    # URLの消去\n",
    "    text = re.sub(r'http?://[\\w/:%#$&\\?~\\.=\\+\\-]+', '', text)\n",
    "    text = re.sub(r'https?://[\\w/:%#$&\\?~\\.=\\+\\-]+', '', text)\n",
    "\n",
    "    # 絵文字の消去\n",
    "    text = demoji.replace(string=text, repl='')\n",
    "\n",
    "    # 文字の正規化\n",
    "    text = neologdn.normalize(text)\n",
    "\n",
    "    # 数字をすべて0に\n",
    "    text = re.sub(r'\\d+', '0', text)\n",
    "\n",
    "    # 大文字を小文字に\n",
    "    text = text.lower()\n",
    "\n",
    "    # 【関連記事, 関連サイト, 関連リンク】以降の消去\n",
    "    target_list = ['関連記事', '関連サイト', '関連リンク']\n",
    "    for target in target_list:\n",
    "      idx = text.find(target)\n",
    "      text = text[:(idx-1)]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "リドリー・スコット監督最新作『プロメテウス』のワールド・プレミアが、現地時間0月0日に、ロンドンのレスター・スクウェアにて開催された。エリザベス女王陛下即位0周年記念に沸き、至る所に国旗が飾られているロンドン。中心街レスター・スクウェアに、鮮やかなブルーカーペットが敷き詰められて、ワールド・プレミアは行われた。今まで一切ストーリーが明かされず、その全貌が謎に包まれたままだった本作の初披露とあって、会場には約0,0名のファンが集結した。『プロメテウス』は、人類史上最大の謎“人類の起源\"を解き明かす重大なヒントを地球上の古代遺跡で発見し、宇宙船プロメテウス号に乗って未知の惑星を訪れた科学者チームの、想像を絶する運命を映し出す。めくるめく神秘と衝撃に彩られた探査航海の果てに、決して触れてはならない“パンドラの箱\"が開いたとき、まだ何も知らない人類はすべてを目撃する。前人未踏の宇宙の彼方に、地球上のあらゆる歴史や文明の概念さえも覆す、驚愕の真実が眠っていたことをー。出演は、『ミレニアムドラゴン・タトゥーの女』で強烈な演技を見せたノオミ・ラパス、日本でも多くの女性たちから圧倒的な支持を受けるハリウッドを代表するオスカー女優シャーリーズ・セロン、『shame-シェイム-』でヴェネチア国際映画祭主演男優賞を獲得し、マグニート役で『x-men:ファースト・ジェネレーション』を崇高なドラマにまで昇華させた俳優マイケル・ファスベンダーらが集う。圧倒的なオリジナリティに満ちた先見性と、アーティスティックな映像感覚で名高いスコット監督は、いま最も多くの注目を集めている彼ら豪華布陣を配し、観る者の好奇心や空想をはるかに超越した“人類の起源\"をビジュアル化した。主要キャストのノオミ・ラパス、シャーリーズ・セロン、マイケル・ファスベンダー、そして巨匠リドリー・スコット監督が会場に登場するとファンの熱気に包まれた。ノオミ・ラパスは胸元の空いた黒いドレス、シャーリーズ・セロンは金色の髪によく似合う青のドレスで集まった人々を魅了。写真撮影と数台のカメラの取材に応えた後、ファンサービスに終始していた。映画『プロメテウス』は、0月0日(金)全国ロードショー<0d/0d同時上映>・映画『プロメテウス』オフィシャ\n"
     ]
    }
   ],
   "source": [
    "preprocessed_text = text_preprocess(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "リドリー・スコット監督最新作『プロメテウス』のワールド・プレミアが、現地時間0月0日に、ロンドンのレスター・スクウェアにて開催された。エリザベス女王陛下即位0周年記念に沸き、至る所に国旗が飾られている\n",
      "ロンドン。中心街レスター・スクウェアに、鮮やかなブルーカーペットが敷き詰められて、ワールド・プレミアは行われた。今まで一切ストーリーが明かされず、その全貌が謎に包まれたままだった本作の初披露とあって、\n",
      "会場には約0,0名のファンが集結した。『プロメテウス』は、人類史上最大の謎“人類の起源\"を解き明かす重大なヒントを地球上の古代遺跡で発見し、宇宙船プロメテウス号に乗って未知の惑星を訪れた科学者チームの\n",
      "、想像を絶する運命を映し出す。めくるめく神秘と衝撃に彩られた探査航海の果てに、決して触れてはならない“パンドラの箱\"が開いたとき、まだ何も知らない人類はすべてを目撃する。前人未踏の宇宙の彼方に、地球上\n",
      "のあらゆる歴史や文明の概念さえも覆す、驚愕の真実が眠っていたことをー。出演は、『ミレニアムドラゴン・タトゥーの女』で強烈な演技を見せたノオミ・ラパス、日本でも多くの女性たちから圧倒的な支持を受けるハリ\n",
      "ウッドを代表するオスカー女優シャーリーズ・セロン、『shame-シェイム-』でヴェネチア国際映画祭主演男優賞を獲得し、マグニート役で『x-\n",
      "men:ファースト・ジェネレーション』を崇高なドラマにまで昇華させた俳優マイケル・ファスベンダーらが集う。圧倒的なオリジナリティに満ちた先見性と、アーティスティックな映像感覚で名高いスコット監督は、い\n",
      "ま最も多くの注目を集めている彼ら豪華布陣を配し、観る者の好奇心や空想をはるかに超越した“人類の起源\"をビジュアル化した。主要キャストのノオミ・ラパス、シャーリーズ・セロン、マイケル・ファスベンダー、そ\n",
      "して巨匠リドリー・スコット監督が会場に登場するとファンの熱気に包まれた。ノオミ・ラパスは胸元の空いた黒いドレス、シャーリーズ・セロンは金色の髪によく似合う青のドレスで集まった人々を魅了。写真撮影と数台\n",
      "のカメラの取材に応えた後、ファンサービスに終始していた。映画『プロメテウス』は、0月0日(金)全国ロードショー<0d/0d同時上映>・映画『プロメテウス』オフィシャ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_wrap_list = textwrap.wrap(''.join(preprocessed_text), 100)\n",
    "print('\\n'.join(s_wrap_list), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
