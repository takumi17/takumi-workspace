# A Survey of Quantization Methods for Efficient Neural Network Inference(効率的なニューラルネットワーク推論のための量子化手法に関する調査) 


## BASIC CONCEPTS OF QUANTIZATION(量子化の基本概念)  
このセクションでは、まずセクションIII-Aで一般的な記法と問題の設定を簡単に紹介し、その後セクションIII-BからIII-Fで基本的な量子化の概念と方法について説明します。その後、セクションIII-Gで異なるファインチューニング方法について議論し、セクションIII-Hで確率的量子化について説明します。

図1: 一様量子化（左）と非一様量子化（右）の比較。連続ドメインの実値 $r$ が、量子化ドメインの離散的で低精度の値 $Q$ にマッピングされており、これらの値はオレンジの点で示されています。一様量子化では量子化レベル間の距離は同じですが、非一様量子化では距離が異なることがあります。

A. 問題設定と表記法
ニューラルネットワーク(NN)が $L$ 層の学習可能なパラメータ $\{W_{1}, W_{2}, \ldots, W_{L}\}$ を持ち、$\theta$ がこれら全てのパラメータの組み合わせを示すと仮定します。一般性を失わずに、ここでは教師あり学習の問題に焦点を当てます。その目的は、次の経験リスク最小化関数を最適化することです。

$$
\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^{N}\mathcal{l}(x_{i}, y_{i};\theta)
$$

ここで、$(x, y)$ は入力データと対応するラベルを示し、$\mathcal{l}(x, y; θ)$ は損失関数（例えば、平均二乗誤差や交差エントロピー損失）を示します。$N$ はデータポイントの総数です。また、$i^{th}$ 番目の層の入力隠れアクティベーションを $h_{i}$、対応する出力隠れアクティベーションを $a_{i}$ と表記します。トレーニング済みのモデルパラメータ $\theta$ は浮動小数点精度で保存されていると仮定します。量子化の目的は、パラメータ $(\theta)$ と中間アクティベーションマップ（すなわち、$h_{i}$ と $a_{i}$）の両方の精度を低精度に減少させ、モデルの一般化能力や精度に最小限の影響を与えることです。これを実現するためには、浮動小数点値を量子化された値にマッピングする量子化演算子を定義する必要があります。これについては次に説明します。

B. 一様量子化
まず、ニューラルネットワーク(NN)の重みとアクティベーションを有限の値の集合に量子化できる関数を定義する必要があります。この関数は、浮動小数点の実数値を取り、それを低精度の範囲にマッピングします。これについては図1で示されています。量子化関数の一般的な選択肢は以下の通りです。

$$
Q(r) = \text{Int}(r/S) - Z
$$

ここで、$Q$ は量子化演算子、$r$ は実数値の入力（アクティベーションまたは重み）、$S$ は実数値のスケーリングファクター、$Z$ は整数のゼロポイントです。さらに、$\text{Int}$ 関数は実数値を整数値に変換するための丸め操作（例えば、最も近い整数への丸めや切り捨て）を行います。基本的に、この関数は実数値 $r$ をいくつかの整数値にマッピングします。この量子化の方法は一様量子化としても知られており、結果として得られる量子化値（すなわち量子化レベル）が均等に間隔を空けて配置されます（図1、左）。また、量子化値が必ずしも均等に配置されていない非一様量子化の方法もあり（図1、右）、これについてはセクション III-F で詳しく説明されます。量子化された値 $Q(r)$ から実数値 r を回復するための操作は、一般にデ量子化と呼ばれます。

注意してください。回復された実数値 $\tilde{r}$ は、丸め操作のために元の実数値 $r$ と正確には一致しないことがあります。

図2: 対称的量子化と非対称的量子化の図解。対称的量子化では制限された範囲が実数値を [-127, 127] にマッピングし、フルレンジでは [-128, 127] にマッピングします（8ビット量子化の場合）。

C. 対称的および非対称的量子化
一様量子化において重要な要素の1つは、式2におけるスケーリングファクター $S$ の選択です。このスケーリングファクターは、実数値 r の与えられた範囲を複数の区間に分割します（[113, 133] で議論されています）。

$$
S = \frac{\beta - \alpha}{2^{b} - 1}
$$

ここで、$[\alpha, \beta]$ はクリッピング範囲を示し、これは実数値をクリッピングするための有界範囲です。また、$b$ は量子化ビット幅を示します。したがって、スケーリングファクターを定義するためには、まずクリッピング範囲 $[\alpha, \beta]$ を決定する必要があります。クリッピング範囲を選択するプロセスは、一般的にキャリブレーションと呼ばれます。

簡単な選択肢としては、信号の最小値および最大値をクリッピング範囲として使用する方法があります。すなわち、$\alpha = r_{min}、\beta = r_{max}$ です。このアプローチは非対称的な量子化方式であり、クリッピング範囲が原点に対して対称でないため、$-\alpha \neq \beta$ になります（図2、右参照）。対称的な量子化方式を使用することも可能で、$\alpha = -\beta$ の対称的なクリッピング範囲を選択します。一般的な選択肢としては、信号の最小値および最大値に基づいて選ぶ方法があります: $−\alpha = \beta = max(|r_{max}|, |r_{min}|)$。非対称的な量子化は、対称的な量子化に比べてクリッピング範囲がより狭くなることがよくあります。これは特に、$\text{ReLU}$ 後のように常に非負の値を持つアクティベーションのようにターゲットの重みやアクティベーションが不均衡な場合に重要です。しかし、対称的な量子化を使用すると、式2の量子化関数が簡略化され、ゼロポイントを $Z = 0$ に置き換えることができます。

$$
Q(r) = \text{Int}(\frac{r}{S})
$$

ここでは、スケーリングファクターに対して2つの選択肢があります。「フルレンジ」対称量子化では、スケーリングファクター $S$ は $\frac{2 \cdot \max(|r|)}{2^{n-1}}$（切り捨て丸めモード）として選ばれ、$\text{INT8}$ のフルレンジ $[-128, 127]$ が使用されます。しかし、「制限範囲」では、スケーリングファクター $S$ は $\frac{\max(|r|)}{2^{n-1}-1}$ として選ばれ、範囲は $[-127, 127]$ のみが使用されます。予想通り、フルレンジアプローチの方がより正確です。

対称的量子化は、ゼロポイントをゼロにすることで推論中の計算コストを削減できるため、重みの量子化において広く採用されています [255]。また、実装がより簡単になるという利点もあります。ただし、アクティベーションについては、非対称アクティベーションのオフセットによって生じる交差項は静的なデータ非依存項であり、バイアスに吸収することができます（またはアキュムレータの初期化に使用することができます） [15]。

信号の最小値/最大値を使用する方法は、対称的および非対称的量子化で一般的ですが、このアプローチはアクティベーションにおける外れ値に対して脆弱です。外れ値が範囲を不必要に広げ、その結果、量子化の解像度が低下する可能性があります。この問題に対処するための $1$ つのアプローチは、信号の最小値/最大値の代わりにパーセンタイルを使用することです [172]。つまり、最大値/最小値の代わりに、$i$ 番目に大きい/小さい値を $\beta/\alpha$ として使用します。別のアプローチは、実値と量子化された値の間の $\text{KL}$ ダイバージェンス（情報損失）を最小化するように $\alpha$ と $\beta$ を選択することです [176]。興味のある読者には、さまざまなモデルで異なるキャリブレーション方法が評価されている [255] を参照することをお勧めします。

要約（対称的量子化 vs 非対称的量子化）。対称的量子化は対称的な範囲を使用してクリッピングを分割します。これにより、式2で $Z = 0$ となるため、実装が容易になります。ただし、範囲が歪んで対称でない場合には最適ではありません。このような場合には、非対称的量子化が推奨されます。

図3: 異なる量子化の粒度の図解。レイヤーごとの量子化では、同じレイヤーに属するすべてのフィルタに対して同じクリッピング範囲が適用されます。これにより、分布が狭いチャネル（例えば、図中のフィルタ1）に対しては量子化の解像度が低くなる可能性があります。チャネルごとの量子化を使用すると、異なるクリッピング範囲を異なるチャネルに割り当てることで、より良い量子化解像度を達成することができます。

D. 範囲キャリブレーションアルゴリズム: 静的 vs 動的量子化
