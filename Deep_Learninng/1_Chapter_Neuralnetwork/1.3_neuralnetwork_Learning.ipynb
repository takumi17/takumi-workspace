{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1章　ニューラルネットワークの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークは, 最初に学習を行い, その学習されたパラメータを利用して推論を行う流れが一般的.  \n",
    "推論は, 多クラス分類などの問題に答えを出す作業で, 学習は, 最適なパラメータを見つける作業."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**損失** とは, 正解のデータとニューラルネットワークの出力結果を元に, どれだけ悪いかをスカラとして算出したもの.  \n",
    "ニューラルネットワークの損失を求めるには **損失関数** を使用する.  \n",
    "多クラスの分類の場合は, **交差エントロピー誤差** を用いる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Softmax関数\n",
    "\n",
    "$$\n",
    "y_{k} = \\frac{\\exp(s_k)}{\\sum_{i=1}^{n} \\exp(s_i)}\n",
    "$$\n",
    "\n",
    "$k$ 番目の出力 $y_k$ を求める計算式を表している.  \n",
    "$\\text{Softmax}$ 関数の出力の各要素は $0.0$ より大きく $1.0$ 未満の実数になる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 交差エントロピー誤差\n",
    "\n",
    "$$\n",
    "L = -\\sum_{k} t_{k} \\log{y_k} \\qquad (式1.7)\n",
    "$$\n",
    "\n",
    "$t_{k}$ は $k$ 番目のクラスに対する教師ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log{y_{nk}}\n",
    "$$\n",
    "\n",
    "これは上記の式1.7を $N$ 個分のデータ拡張しただけ.  \n",
    "$1$ 個あたりの損失関数を求めることで, 統一した指標が得られる.\n",
    "$\\text{Softmax}$ 関数と交差エントロピー誤差を計算するレイヤを $\\text{Sofftmax with Loss}$ レイヤとして実装する."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 微分と勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの学習の目標は, 損失をできるだけ少なくするパラメータを見つける.  \n",
    "この時重要になるのが, 微分と勾配.  \n",
    "$L$ をスカラ, $\\mathbf{x}$ をベクトルとして, $L = f(\\mathbf{x})$ という関数があるとする.  \n",
    "この時の $\\mathbf{x}$ の偏微分は,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{x}}\n",
    "=\\left(\\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}, \\cdots , \\frac{\\partial L}{\\partial x_n}\\right)\n",
    "$$\n",
    "\n",
    "これを勾配という.  \n",
    "$\\mathbf{W}$ を $m\\times n$ の行列とすると, $L = g(\\mathbf{W})$ の勾配は, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial W_{11}} & \\cdots & \\frac{\\partial L}{\\partial W_{1n}} \\\\\n",
    "\\vdots & \\ddots & \\\\\n",
    "\\frac{\\partial L}{\\partial W_{m1}} &  & \\frac{\\partial L}{\\partial x_{mn}}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 チェインルール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**誤差逆伝播法**　を理解する上で, 大事なのは　**チェインルール(連鎖律)**　である.  \n",
    "これは, 合成関数に関する微分の法則である.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4.3 Repeatノード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D)\n",
    "y = np.repeat(x, N, axis=0)\n",
    "#print(x)\n",
    "#print(y)\n",
    "\n",
    "dy = np.random.randn(N, D)\n",
    "dx = np.sum(dy, axis=0, keepdims=True)\n",
    "#print(dy)\n",
    "#print(dx)\n",
    "\n",
    "#dx = np.sum(dy, axis=0, keepdims=False)\n",
    "#print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.repeat()` は, 要素の複製を行う.  \n",
    "`axis=0` は列に沿った操作, `axis=1` は行に沿った操作.  \n",
    "`keepdims=True` は, 結果の配列が元の配列 `dy` と同じ数の次元をもつ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4.4 Sumノード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(N, D)\n",
    "y = np.sum(x, axis=0, keepdims=True)\n",
    "\n",
    "dy = np.random.randn(1, D)\n",
    "dx = np.repeat(x, N, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumノードの順伝播は `np.sum()` メソッド, 逆伝播は, `np.repeat()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4.5 MatMulノード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `params`: 重みパラメータをリストで保持します。この例では、1つの重み行列 `W` が要素として入ている\n",
    "- `grads`: 勾配(gradient)をリストで保持.  `W` に対応する勾配が要素として入る.\n",
    "- `x`: 入力データ(forwardで受け取ったデータ)を保持するための変数.  \n",
    "\n",
    "- `__init__(self, W)`: 引数 `W` として重み行列を受け取り, `params` と `grads` を初期化. 同時に, 入力データ保持用の `x` は `None` に初期化.\n",
    "\n",
    "- `forward(self, x)`: 順伝播(forward propagation)処理を行うメソッド.\n",
    "    - 最初に, `params` から重み行列 `W` を取り出す.\n",
    "    - 入力データ `x` と重み行列 `W` の行列積を計算して、出力を `out` に代入.\n",
    "    - 入力データ `x` を `self.x` に保持する.\n",
    "    - 計算した出力を返す.\n",
    "\n",
    "- `backward(self, dout)`: 逆伝播(backward propagation)処理を行うメソッド.\n",
    "    - 最初に、`params` から重み行列 `W` を取り出す.\n",
    "    - 出力に対する勾配 `dout` と、重み行列 `W` の転置行列との行列積を計算して、入力に対する勾配 `dx` を求める.\n",
    "    - 入力データ `self.x` の転置行列と、出力に対する勾配 `dout` の行列積を計算して、重み行列 `W` の勾配 `dW` を求める.\n",
    "    - `grads` の最初の要素 (重み行列に対応する勾配) を、計算した `dW` で更新.\n",
    "    - 計算した入力に対する勾配 `dx` を返す."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
